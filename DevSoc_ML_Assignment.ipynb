{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nclass Linear:\n    def __init__(self, in_features, out_features):\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weights = np.random.randn(in_features, out_features) * 0.01\n        self.biases = np.zeros((1, out_features))\n        self.input = None\n        self.grad_weights = None\n        self.grad_biases = None\n\n    def forward(self, x):\n        self.input = x\n        return np.dot(x, self.weights) + self.biases\n\n    def backward(self, d_out):\n        self.grad_weights = np.dot(self.input.T, d_out)\n        self.grad_biases = np.sum(d_out, axis=0, keepdims=True)\n        d_input = np.dot(d_out, self.weights.T)\n        return d_input\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-11T18:19:00.668024Z","iopub.execute_input":"2024-08-11T18:19:00.668444Z","iopub.status.idle":"2024-08-11T18:19:00.679196Z","shell.execute_reply.started":"2024-08-11T18:19:00.668410Z","shell.execute_reply":"2024-08-11T18:19:00.677997Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"class ReLU:\n    def forward(self, x):\n        self.input = x\n        return np.maximum(0, x)\n\n    def backward(self, d_out):\n        d_input = d_out.copy()\n        d_input[self.input <= 0] = 0\n        return d_input\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.681573Z","iopub.execute_input":"2024-08-11T18:19:00.681989Z","iopub.status.idle":"2024-08-11T18:19:00.693125Z","shell.execute_reply.started":"2024-08-11T18:19:00.681955Z","shell.execute_reply":"2024-08-11T18:19:00.692003Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"class Sigmoid:\n    def forward(self, x):\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, d_out):\n        return d_out * (self.output * (1 - self.output))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.694452Z","iopub.execute_input":"2024-08-11T18:19:00.694820Z","iopub.status.idle":"2024-08-11T18:19:00.712513Z","shell.execute_reply.started":"2024-08-11T18:19:00.694788Z","shell.execute_reply":"2024-08-11T18:19:00.711426Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"class Tanh:\n    def forward(self, x):\n        self.output = np.tanh(x)\n        return self.output\n\n    def backward(self, d_out):\n        return d_out * (1 - self.output ** 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.714099Z","iopub.execute_input":"2024-08-11T18:19:00.714549Z","iopub.status.idle":"2024-08-11T18:19:00.726057Z","shell.execute_reply.started":"2024-08-11T18:19:00.714505Z","shell.execute_reply":"2024-08-11T18:19:00.725044Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"class Softmax:\n    def forward(self, x):\n        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, d_out):\n        # Assuming d_out is already the gradient of loss w.r.t. softmax output\n        return d_out\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.729242Z","iopub.execute_input":"2024-08-11T18:19:00.729687Z","iopub.status.idle":"2024-08-11T18:19:00.739351Z","shell.execute_reply.started":"2024-08-11T18:19:00.729653Z","shell.execute_reply":"2024-08-11T18:19:00.738164Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"class CrossEntropyLoss:\n    def forward(self, y_pred, y_true):\n        self.y_pred = y_pred\n        self.y_true = y_true\n        self.n = y_true.shape[0]\n        loss = -np.sum(y_true * np.log(y_pred + 1e-10)) / self.n\n        return loss\n\n    def backward(self):\n        return (self.y_pred - self.y_true) / self.y_true.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.741734Z","iopub.execute_input":"2024-08-11T18:19:00.742246Z","iopub.status.idle":"2024-08-11T18:19:00.752233Z","shell.execute_reply.started":"2024-08-11T18:19:00.742203Z","shell.execute_reply":"2024-08-11T18:19:00.750940Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"class MSELoss:\n    def forward(self, y_pred, y_true):\n        self.y_pred = y_pred\n        self.y_true = y_true\n        return np.mean((y_pred - y_true) ** 2)\n\n    def backward(self):\n        return 2 * (self.y_pred - self.y_true) / self.y_true.size\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.753309Z","iopub.execute_input":"2024-08-11T18:19:00.753681Z","iopub.status.idle":"2024-08-11T18:19:00.765665Z","shell.execute_reply.started":"2024-08-11T18:19:00.753637Z","shell.execute_reply":"2024-08-11T18:19:00.764483Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"class SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def step(self, layer):\n        if hasattr(layer, 'grad_weights'):\n            layer.weights -= self.learning_rate * layer.grad_weights\n            layer.biases -= self.learning_rate * layer.grad_biases\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.767167Z","iopub.execute_input":"2024-08-11T18:19:00.767520Z","iopub.status.idle":"2024-08-11T18:19:00.778299Z","shell.execute_reply.started":"2024-08-11T18:19:00.767489Z","shell.execute_reply":"2024-08-11T18:19:00.777149Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"class Model:\n    def __init__(self):\n        self.layers = []\n        self.loss_fn = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        self.layers.append(layer)\n\n    def compile(self, loss_fn, optimizer):\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n\n    def backward(self, loss_grad):\n        for layer in reversed(self.layers):\n            loss_grad = layer.backward(loss_grad)\n\n    def train(self, x_train, y_train, epochs, batch_size):\n        for epoch in range(epochs):\n            # Shuffle training data\n            indices = np.arange(x_train.shape[0])\n            np.random.shuffle(indices)\n            x_train, y_train = x_train[indices], y_train[indices]\n            \n            # Mini-batch training\n            for start in range(0, x_train.shape[0], batch_size):\n                end = min(start + batch_size, x_train.shape[0])\n                x_batch, y_batch = x_train[start:end], y_train[start:end]\n                \n                # Forward pass\n                predictions = self.forward(x_batch)\n                \n                # Compute loss\n                loss = self.loss_fn.forward(predictions, y_batch)\n                \n                # Backward pass\n                loss_grad = self.loss_fn.backward()\n                self.backward(loss_grad)\n                \n                # Update parameters\n                for layer in self.layers:\n                    self.optimizer.step(layer)\n                \n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n\n    def evaluate(self, x_test, y_test):\n        predictions = self.forward(x_test)\n        loss = self.loss_fn.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n        return loss, accuracy\n\n    def save(self, filename):\n        pass\n\n    def load(self, filename):\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.825839Z","iopub.execute_input":"2024-08-11T18:19:00.826592Z","iopub.status.idle":"2024-08-11T18:19:00.845318Z","shell.execute_reply.started":"2024-08-11T18:19:00.826552Z","shell.execute_reply":"2024-08-11T18:19:00.843863Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.utils import to_categorical\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Separate features and labels\nx = train_df.drop(columns='label').values\ny = train_df['label'].values\n\n# Normalize pixel values\nx = x / 255.0\n\n# Reshape data to (num_samples, 28, 28)\nx = x.reshape(-1, 28, 28)\n\n# One-hot encode the labels\ny = to_categorical(y, 10)\n\n# Flatten the data\nx = x.reshape(x.shape[0], -1)\n\n# Split data into training and validation sets\n\nnp.random.seed(42)  \nindices = np.arange(x.shape[0])\nnp.random.shuffle(indices)\nsplit_index = int(0.8 * len(indices))  # 80% for training, 20% for validation\n\ntrain_indices = indices[:split_index]\nval_indices = indices[split_index:]\n\nx_train, x_val = x[train_indices], x[val_indices]\ny_train, y_val = y[train_indices], y[val_indices]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:00.847427Z","iopub.execute_input":"2024-08-11T18:19:00.847850Z","iopub.status.idle":"2024-08-11T18:19:04.616844Z","shell.execute_reply.started":"2024-08-11T18:19:00.847812Z","shell.execute_reply":"2024-08-11T18:19:04.615314Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Initialize model and layers\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\nloss_fn = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.7)\nmodel.compile(loss_fn, optimizer)\n\nmodel.train(x_train, y_train, epochs=20, batch_size=64)\n\nval_loss, val_accuracy = model.evaluate(x_val, y_val)\nprint(f'Validation Loss: {val_loss}')\nprint(f'Validation Accuracy: {val_accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:19:04.618648Z","iopub.execute_input":"2024-08-11T18:19:04.619509Z","iopub.status.idle":"2024-08-11T18:19:31.922186Z","shell.execute_reply.started":"2024-08-11T18:19:04.619462Z","shell.execute_reply":"2024-08-11T18:19:31.920714Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 0.20843550992723342\nEpoch 2/20, Loss: 0.08247531022362248\nEpoch 3/20, Loss: 0.26419405883581965\nEpoch 4/20, Loss: 0.023669446127142335\nEpoch 5/20, Loss: 0.10499128563794523\nEpoch 6/20, Loss: 0.0583920196586511\nEpoch 7/20, Loss: 0.020803659266510476\nEpoch 8/20, Loss: 0.005990714559860159\nEpoch 9/20, Loss: 0.006563529149793313\nEpoch 10/20, Loss: 0.01015675957008653\nEpoch 11/20, Loss: 0.015729013246983305\nEpoch 12/20, Loss: 0.004346433137511537\nEpoch 13/20, Loss: 0.005053163384984366\nEpoch 14/20, Loss: 0.003937922445715305\nEpoch 15/20, Loss: 0.00256072955598431\nEpoch 16/20, Loss: 0.0023850757817414775\nEpoch 17/20, Loss: 0.0014737783023120628\nEpoch 18/20, Loss: 0.0006843251203996258\nEpoch 19/20, Loss: 0.0002722352386597173\nEpoch 20/20, Loss: 0.0005156941904956196\nValidation Loss: 0.10205652933361634\nValidation Accuracy: 0.9772619047619048\n","output_type":"stream"}]}]}